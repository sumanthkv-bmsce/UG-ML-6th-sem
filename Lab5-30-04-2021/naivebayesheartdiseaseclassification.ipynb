{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/heartdiseasedataset/heart (1) (1).csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nimport random\nimport math\n \ndef loadcsv(filename):\n\tlines = csv.reader(open(filename, \"r\"));\n\tdataset = list(lines)\n\tfor i in range(1,len(dataset)):\n       #converting strings into numbers for processing\n\t\tdataset[i] = [float(x) for x in dataset[i]]\n        \n\treturn dataset[1:]\n \ndef splitdataset(dataset, splitratio):\n    #67% training size\n\ttrainsize = int(len(dataset) * splitratio);\n\ttrainset = []\n\tcopy = list(dataset);    \n\twhile len(trainset) < trainsize:\n#generate indices for the dataset list randomly to pick ele for training data\n\t\tindex = random.randrange(len(copy));       \n\t\ttrainset.append(copy.pop(index))    \n\treturn [trainset, copy]\n \ndef separatebyclass(dataset):\n\tseparated = {} #dictionary of classes 1 and 0 \n#creates a dictionary of classes 1 and 0 where the values are \n#the instances belonging to each class\n\tfor i in range(len(dataset)):\n\t\tvector = dataset[i]\n\t\tif (vector[-1] not in separated):\n\t\t\tseparated[vector[-1]] = []\n\t\tseparated[vector[-1]].append(vector)\n\treturn separated\n \ndef mean(numbers):\n\treturn sum(numbers)/float(len(numbers))\n \ndef stdev(numbers):\n\tavg = mean(numbers)\n\tvariance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n\treturn math.sqrt(variance)\n \ndef summarize(dataset): #creates a dictionary of classes\n\tsummaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)];\n\tdel summaries[-1] #excluding labels +ve or -ve\n\treturn summaries\n \ndef summarizebyclass(dataset):\n\tseparated = separatebyclass(dataset); \n    #print(separated)\n\tsummaries = {}\n\tfor classvalue, instances in separated.items(): \n#for key,value in dic.items()\n#summaries is a dic of tuples(mean,std) for each class value        \n\t\tsummaries[classvalue] = summarize(instances) #summarize is used to cal to mean and std\n\treturn summaries\n \ndef calculateprobability(x, mean, stdev):\n\texponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n\treturn (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n \ndef calculateclassprobabilities(summaries, inputvector):\n\tprobabilities = {} # probabilities contains the all prob of all class of test data\n\tfor classvalue, classsummaries in summaries.items():#class and attribute information as mean and sd\n\t\tprobabilities[classvalue] = 1\n\t\tfor i in range(len(classsummaries)):\n\t\t\tmean, stdev = classsummaries[i] #take mean and sd of every attribute for class 0 and 1 seperaely\n\t\t\tx = inputvector[i] #testvector's first attribute\n\t\t\tprobabilities[classvalue] *= calculateprobability(x, mean, stdev);#use normal dist\n\treturn probabilities\n\t\t\t\ndef predict(summaries, inputvector): #training and test data is passed\n\tprobabilities = calculateclassprobabilities(summaries, inputvector)\n\tbestLabel, bestProb = None, -1\n\tfor classvalue, probability in probabilities.items():#assigns that class which has he highest prob\n\t\tif bestLabel is None or probability > bestProb:\n\t\t\tbestProb = probability\n\t\t\tbestLabel = classvalue\n\treturn bestLabel\n \ndef getpredictions(summaries, testset):\n\tpredictions = []\n\tfor i in range(len(testset)):\n\t\tresult = predict(summaries, testset[i])\n\t\tpredictions.append(result)\n\treturn predictions\n \ndef getaccuracy(testset, predictions):\n\tcorrect = 0\n\tfor i in range(len(testset)):\n\t\tif testset[i][-1] == predictions[i]:\n\t\t\tcorrect += 1\n\treturn (correct/float(len(testset))) * 100.0\n \ndef main():\n\tfilename = '../input/heartdiseasedataset/heart (1) (1).csv'\n\tsplitratio = 0.67\n\tdataset = loadcsv(filename);\n     \n\ttrainingset, testset = splitdataset(dataset, splitratio)\n\tprint('Split {0} rows into train={1} and test={2} rows'.format(len(dataset), len(trainingset), len(testset)))\n\t# prepare model\n\tsummaries = summarizebyclass(trainingset);    \n\t#print(summaries)\n    # test model\n\tpredictions = getpredictions(summaries, testset) #find the predictions of test data with the training data\n\taccuracy = getaccuracy(testset, predictions)\n\tprint('Accuracy of the classifier is : {0}%'.format(accuracy))\n \nmain()","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Split 303 rows into train=203 and test=100 rows\nAccuracy of the classifier is : 83.0%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}